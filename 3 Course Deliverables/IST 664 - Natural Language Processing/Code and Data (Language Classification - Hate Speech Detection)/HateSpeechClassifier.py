# -*- coding: utf-8 -*-
"""HateSpeechClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yASfm3Ummk_sRE1c5_m5fx_zhu7HMvss
"""

import pandas as pd
import html
import re
# from sklearn.feature_extraction.text import CountVectorizer
import nltk
import random
# import numpy as np
from nltk.tokenize import TweetTokenizer
nltk.download('stopwords')
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_curve
nltk.download('punkt')
import datetime

"""# Functions & Data

### def classify(feature_sets):
"""

def classify(feature_sets):
  k_folds = 10
  test_size = 0.3
  split_point = int(round(len(feature_sets)*(1-test_size),0))
  X = []
  y = []
  for example in feature_sets:
    X.append(example[0])
    y.append(example[1])
  X = pd.DataFrame(X).values
  train_X, test_X = X[:split_point], X[split_point:]
  train_y, test_y = y[:split_point], y[split_point:]

  print('Train MNB model on {} examples with {}-fold cross-validation'.format(len(y), k_folds))
  classifier = MultinomialNB()
  classifier.fit(X, y)
  y_pred = cross_val_predict(classifier, X, y, cv=k_folds)
  cnf = confusion_matrix(y, y_pred)

  print('Confusion Matrix:\n',cnf)
  tn = cnf[0][0]
  tp = cnf[1][1]
  fn = cnf[1][0]
  fp = cnf[0][1]
  n_obs = sum(sum(cnf))
  acc = (tn+tp)/n_obs
  p_1 = tp/(tp+fp)
  r_1 = tp/(tp+fn)
  f1_1 = (2*p_1*r_1)/(p_1+r_1)
  print('Accuracy:',round(acc,4))
  print('Precision (1):',round(p_1,4))
  print('Recall (1):',round(r_1,4))
  print('F1 (1):',round(f1_1,4))

  features = list(feature_sets[0][0].keys())
  log_prob_diffs = []
  for i in range(len(features)):
    log_prob_diff = classifier.feature_log_prob_[0][i] - classifier.feature_log_prob_[1][i]
    log_prob_diffs.append(log_prob_diff)
  feature_ranks = sorted(zip(log_prob_diffs, features))

  print_list = ['\nPositive Indicators', '\nNegative Indicators']
  for i in range(2):
    print(print_list[i])
    for j in range(20):
      if i == 0:
        print(feature_ranks[j])
      else:
        print(feature_ranks[-(j+1)])
        
  pred = pd.DataFrame()
  pred['actual'] = y
  pred['pred'] = y_pred
  pred = pred.join(df_orig['tweet'].reset_index(drop=True), how='inner')
  errors = pred[pred['actual'] != pred['pred']]
  sample_errors = errors.sample(20).reset_index(drop=True)
  print('\nErrors:')
  for i in range(sample_errors.shape[0]):
    print(sample_errors.iloc[i]['actual'],'\t',sample_errors.iloc[i]['tweet'])

"""### Import & Prep Data"""

df_orig = pd.read_csv('https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv')
df_orig = df_orig[['class','tweet']]
df_orig['class'] = df_orig['class'].apply(lambda x: 1 if x == 0 else 0) # translate class from {0: 'hate speech', 1: 'offensive language', 2: 'neither'} to {0: 'not hate speech', 1: 'hate speech'}
df_orig['tweet'] = df_orig['tweet'].apply(html.unescape) # convert emojis from html
df_orig = df_orig.sample(frac=1, random_state=3).reset_index(drop=True)

print(df_orig.head(10).to_string())

"""# Feature Extraction & Experiments"""

print('Baseline Accuracy: '+str(round((1-df_orig['class'].mean())*100, 2))+'%')

"""## Feature functions"""

def word_features(document, common_words):
    document_words = set(document)
    features = {}
    for word in common_words:
        features['V_{}'.format(word)] = (word in document_words)
    return features

def get_ngrams(document, n_min=1, n_max=None, stopwords=None):
  if n_max is None:
    n_max = n_min
  document_ngrams = []
  for i in range(n_min, n_max+1):
    document_ngrams.extend(list(nltk.ngrams(document,i)))
  if stopwords is not None:
    for ngram in document_ngrams.copy():
      for token in ngram:
        if token in stopwords:
          document_ngrams.remove(ngram)
          break
  return document_ngrams

def get_all_ngrams(collection, n_min=1, n_max=None, stopwords=None):
  if n_max is None:
    n_max = n_min
  collection_ngrams = []
  for (cat, doc) in collection:
    collection_ngrams.extend(get_ngrams(doc, n_min, n_max, stopwords))
  return collection_ngrams

def word_and_ngram_features(document, common_words, common_ngrams, stopwords):
  document_words = set(document)
  document_ngrams = get_ngrams(document,2,4,stopwords)
  features = {}
  for word in common_words:
    features['V_{}'.format(word)] = (word in document_words)
  for ngram in common_ngrams:
    features['N_{}'.format('_'.join(ngram))] = (ngram in document_ngrams)
  return features

def get_hate_score(tweet, calc):
  tweet_tokens = tokenize.tokenize(tweet)
  tweet_score = 0
  tweet_terms = []
  for (term, score) in hatebase_items:
    if re.search(r'\b({})\b'.format(term), tweet):
      tweet_score += score
      tweet_terms.append(term)
  if calc == 1:
    tweet_score = tweet_score/len(tweet_tokens)
  elif calc == 2:
    try:
      tweet_score = tweet_score/len(tweet_terms)
    except:
      tweet_score = 0
  return tweet_score

def hate_score_feature(feature_sets, calc=1):
  i = 0
  start_time = datetime.datetime.now()
  i0_time = start_time
  for (features, cat) in feature_sets:
    tweet = df['tweet'][i]
    tweet_hate_score = get_hate_score(tweet, calc)
    features['HATE_SCORE'] = tweet_hate_score
    i += 1
    if i%1000 == 0:
      i1_time = datetime.datetime.now()
      print(str(i)+'\t'+str(i1_time-i0_time)+'\t'+str(i1_time-start_time))
      i0_time = i1_time
  total_time = datetime.datetime.now()-start_time
  print(total_time)
  return feature_sets

"""## Experiment 1
Top 2000 of all words as features

### Prep
"""

tokenize = TweetTokenizer()
df_tokens = df_orig.copy(deep=True)
df_tokens['tweet'] = df_tokens['tweet'].apply(lambda x: tokenize.tokenize(x))
docs = df_tokens.to_records(index=False)

print(docs[0:10])

all_words_list = [word for (cat,tweet) in docs for word in tweet]
all_words = nltk.FreqDist(all_words_list)
print(len(all_words))

all_words = nltk.FreqDist(all_words_list)

# get the most frequently appearing keywords in the corpus
common_word_items = all_words.most_common(2000)
common_words = [word for (word,count) in common_word_items]

feature_sets = [(word_features(tweet, common_words), cat) for (cat, tweet) in docs]
print(feature_sets[0])

"""### Results"""

classify(feature_sets)

"""## Experiment 2
Top 2000 most frequent words with additional cleanup

### Prep
"""

df = df_orig.copy(deep=True)

df['tweet'] = df['tweet'].apply(lambda x: x.lower())
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'@[A-z0-9_]{1,15}','USERNAME_MENTION', x)) # replace username mentions with generic placeholder; usernames are alpha-numeric + underscores and not longer than 15 characters (https://help.twitter.com/en/managing-your-account/twitter-username-rules)
df['tweet'] = df['tweet'].apply(lambda x: x.replace(r'"USERNAME_MENTION:', '"RETWEET_SOURCE:')) # replace retweet sources with generic placeholder; USERNAME_MENTION preceded by " and followed by :
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'http(s)?://[A-z0-9\-._~:/?#\[\]@!$&\'()*+,;=]+','URL_LINK', x)) # replace URLs with generic placeholder

tokenize = TweetTokenizer()
df_tokens = df.copy(deep=True)
df_tokens['tweet'] = df_tokens['tweet'].apply(lambda x: tokenize.tokenize(x))
docs = df_tokens.to_records(index=False)

print(docs[0:10])

all_words_list = [word for (cat,tweet) in docs for word in tweet]
all_words = nltk.FreqDist(all_words_list)
print(len(all_words))

all_words = nltk.FreqDist(all_words_list)

# get the most frequently appearing keywords in the corpus
common_word_items = all_words.most_common(2000)
common_words = [word for (word,count) in common_word_items]

feature_sets = [(word_features(tweet, common_words), cat) for (cat, tweet) in docs]
print(feature_sets[0])

"""### Results"""

classify(feature_sets)

"""## Experiment 3
Top 2000 words with even more cleanup

### Prep
"""

df = df_orig.copy(deep=True)

df['tweet'] = df['tweet'].apply(lambda x: x.lower())
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'@[A-z0-9_]{1,15}','USERNAME_MENTION', x)) # replace username mentions with generic placeholder; usernames are alpha-numeric + underscores and not longer than 15 characters (https://help.twitter.com/en/managing-your-account/twitter-username-rules)
df['tweet'] = df['tweet'].apply(lambda x: x.replace(r'"USERNAME_MENTION:', '"RETWEET_SOURCE:')) # replace retweet sources with generic placeholder; USERNAME_MENTION preceded by " and followed by :
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'http(s)?://[A-z0-9\-._~:/?#\[\]@!$&\'()*+,;=]+','URL_LINK', x)) # replace URLs with generic placeholder

# replace common abbreviated words
df['tweet'] = df['tweet'].apply(lambda x: x.replace('\n', ' '))
df['tweet'] = df['tweet'].apply(lambda x: x.replace('&', ' and '))
df['tweet'] = df['tweet'].apply(lambda x: x.replace('’', '\''))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'“|”', '"', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'…|\.{2,}', '...', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bbout\b', ' about ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\br\b', ' are ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bb\b', ' be ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bcan\'?t\b', ' cannot ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bdon\'?t\b', ' do not ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bhe\'?s\b', ' he is ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bi\'?m\b', ' i am ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bpls\b', ' please ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bplz\b', ' please ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bshe\'s\b', ' she is ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bdat\b', ' that ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bda\b', ' the ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bwon\'?t\b', ' will not ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bw/\b', ' with ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bu\b', ' you ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bur\b', ' your ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\byou\'?re\b', ' you are ', x))

tokenize = TweetTokenizer()
df_tokens = df.copy(deep=True)
df_tokens['tweet'] = df_tokens['tweet'].apply(lambda x: tokenize.tokenize(x))
docs = df_tokens.to_records(index=False)

print(docs[0:10])

all_words_list = [word for (cat,tweet) in docs for word in tweet]
all_words = nltk.FreqDist(all_words_list)
print(len(all_words))

all_words = nltk.FreqDist(all_words_list)

# get the most frequently appearing keywords in the corpus
common_word_items = all_words.most_common(2000)
common_words = [word for (word,count) in common_word_items]
print(common_words)

feature_sets = [(word_features(tweet, common_words), cat) for (cat, tweet) in docs]
print(feature_sets[0])

"""### Results"""

classify(feature_sets)

"""## Experiment 4
Top 2000 most frequent words after cleaning and stopwords removed

### Prep
"""

all_words_list = [word for (cat,tweet) in docs for word in tweet]
all_words = nltk.FreqDist(all_words_list)
print(len(all_words))

# remove stop words
stopwords = nltk.corpus.stopwords.words('english')
stopwords.extend(['.',',','rt','-',':'])
all_words_list = [word for word in all_words_list if word not in stopwords]
all_words = nltk.FreqDist(all_words_list)
print(len(all_words))

# get the most frequently appearing keywords in the corpus
common_word_items = all_words.most_common(2000)
common_words = [word for (word,count) in common_word_items]
print(common_words)

feature_sets = [(word_features(tweet, common_words), cat) for (cat, tweet) in docs]
print(feature_sets[0])

"""### Results"""

classify(feature_sets)

"""## Experiment 5
Top 2000 most frequent words after cleaning and stopwords removed (except pronouns)

### Prep
"""

all_words_list = [word for (cat,tweet) in docs for word in tweet]
all_words = nltk.FreqDist(all_words_list)
print(len(all_words))

# remove stop words
stopwords = nltk.corpus.stopwords.words('english')
stopwords.extend(['.',',','rt','-',':'])
stopwords = stopwords[0:8]+stopwords[26:]
all_words_list = [word for word in all_words_list if word not in stopwords]
all_words = nltk.FreqDist(all_words_list)
print(len(all_words))

# get the most frequently appearing keywords in the corpus
common_word_items = all_words.most_common(2000)
common_words = [word for (word,count) in common_word_items]
print(common_words)

feature_sets = [(word_features(tweet, common_words), cat) for (cat, tweet) in docs]
print(feature_sets[0])

"""### Results"""

classify(feature_sets)

"""## Experiment 6
Add bigrams as features

### Prep
"""

df = df_orig.copy(deep=True)

df['tweet'] = df['tweet'].apply(lambda x: x.lower())
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'@[A-z0-9_]{1,15}','USERNAME_MENTION', x)) # replace username mentions with generic placeholder; usernames are alpha-numeric + underscores and not longer than 15 characters (https://help.twitter.com/en/managing-your-account/twitter-username-rules)
df['tweet'] = df['tweet'].apply(lambda x: x.replace(r'"USERNAME_MENTION:', '"RETWEET_SOURCE:')) # replace retweet sources with generic placeholder; USERNAME_MENTION preceded by " and followed by :
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'http(s)?://[A-z0-9\-._~:/?#\[\]@!$&\'()*+,;=]+','URL_LINK', x)) # replace URLs with generic placeholder

# replace common abbreviated words
df['tweet'] = df['tweet'].apply(lambda x: x.replace('\n', ' '))
df['tweet'] = df['tweet'].apply(lambda x: x.replace('&', ' and '))
df['tweet'] = df['tweet'].apply(lambda x: x.replace('’', '\''))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'“|”', '"', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'…|\.{2,}', '...', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bbout\b', ' about ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\br\b', ' are ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bb\b', ' be ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bcan\'?t\b', ' cannot ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bdon\'?t\b', ' do not ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bhe\'?s\b', ' he is ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bi\'?m\b', ' i am ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bpls\b', ' please ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bplz\b', ' please ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bshe\'s\b', ' she is ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bdat\b', ' that ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bda\b', ' the ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bwon\'?t\b', ' will not ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bw/\b', ' with ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bu\b', ' you ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\bur\b', ' your ', x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\byou\'?re\b', ' you are ', x))

# replace repeated mentions with single mention
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'(\s?USERNAME_MENTION\s?){2,}', ' USERNAME_MENTION ', x))

tokenize = TweetTokenizer(reduce_len=True)
df_tokens = df.copy(deep=True)
df_tokens['tweet'] = df_tokens['tweet'].apply(lambda x: tokenize.tokenize(x))
docs = df_tokens.to_records(index=False)

print(docs[0:10])

stopwords = nltk.corpus.stopwords.words('english')
stopwords.extend(['.',',','rt','-',':'])
stopwords = stopwords[0:8]+stopwords[26:]

# most common words without stopwords
all_words_list = [word for (cat,tweet) in docs for word in tweet]
all_words_list = [word for word in all_words_list if word not in stopwords]
all_words = nltk.FreqDist(all_words_list)
common_word_items = all_words.most_common(2000)
common_words = [word for (word,count) in common_word_items]
# print(common_words)

# most common ngrams without stopwords
all_ngrams = get_all_ngrams(docs, 2, 4, stopwords)
ngram_fd = nltk.FreqDist(all_ngrams)
common_ngram_items = ngram_fd.most_common(1000)
common_ngrams = [ngram for (ngram,count) in common_ngram_items]

feature_sets = [(word_and_ngram_features(tweet, common_words, common_ngrams, stopwords), cat) for (cat, tweet) in docs]
print(feature_sets[0])

"""### Results"""

classify(feature_sets)

"""## Experiment 7
average offensiveness scores (calc 1)

### Prep
"""

hatebase = pd.read_csv('hatebase.csv').dropna().reset_index(drop=True)
hatebase_items = []
for i in range(hatebase.shape[0]):
  hatebase_items.append((hatebase['term'][i], hatebase['average_offensiveness'][i]))

feature_sets = hate_score_feature(feature_sets, calc=1)

classify(feature_sets)

"""## Experiment 8
average offensiveness scores (calc 2)

### Prep
"""

feature_sets = hate_score_feature(feature_sets, calc=2)

"""### Results"""

classify(feature_sets)